---
layout: post
title:  "앞으로 데이터 분석을 시작하려는 사람을 위한 위한 책"
date:   2017-03-08 11:26:20 +0900
tags: databook introdata
comments: true
---

# 도서
{:.no_toc}

![cover]({{ site.baseurl}}/assets/start.jpg)
[앞으로 데이터 분석을 시작하려는 사람을 위한 위한 책](http://www.yes24.com/24/Goods/13047254?Acode=101)

구도 다쿠야(지은이). 김정환(옮긴이). 루비페이퍼. 2014

# 감상
{:.no_toc}
작년 말부터 데이터 분석에 대한 관심을 가지고 있었지만, 누군가 나에게 데이터 분석에 대해 물어보면 뒤죽박죽 이야기가 꺼내지더라. 데이터 분석에 관련해서 여기저기서 정보가 쏟아져나오는데, 이것들이 머리 속에서 매끄럽게 정리되지 않았기 때문이다. 이와중에 이 책을 접하고, 어느정도 그 문제가 해결된 듯하다. 현업실무자가 현실적인 사례를 통해 데이터 분석을 단계별로 설명해주니 정리하기도 쉽고, 현장을 상상해볼 수도 있어 좋았다. 그리고 저자는 경영자 및 현장과의 커뮤니케이션을 강조하는데, 어느 분야나 마찬가지겠지만 데이터 분석가가 문제를 잘 해결하기 위해서는 여러 부서와의 커뮤니케이션이 정말 중요한 것 같다. 단지, 툴을 다루고 모델링하는 기술이 좋다고 되는게 아니라는 것. 책 제목 그대로, 앞으로 데이터 분석을 시작하려는 사람들에게 추천!!

# 기록
{:.no_toc}

* ToC
{:toc}

---

# 서문

> 데이터 분석의 가장 큰 효과는 '의사 결정 프로세스의 최적화'다

# [제 1장] 왜 국내 기업은 데이터 분석에 취약한가

### 일본식 데이터 분석이란 무엇인가

아마존 -> 협업 필터링(Collaborative Filtering): 많은 사용자의 기호 정보와 행동 정보를 축적한 다음 어떤 사용자와 비슷한 행동을 하는 사용자들의 기호 정보를 바탕으로 그 사용자의 기호를 추측하는 방법

디즈니 -> 대기 행렬 이론(Queueing Thoery): 예를 들어 고객이 계산대 앞에서 줄을 서서 기다리지 않고 서비스를 받을 확률, 행렬의 평균 길이, 서비스의 평균 시간 등을 계산하는 이론

통계가 정책을 결정하거나 비즈니스 전략을 자동으로 책정해주는 것은 아니다. 너무나 당연한 말이지만, 과학적 논리라는 축을 중심에 두고 경험과 전문성 있는 견식을 활용해 최종적인 의사 판단을 하는 것이 중요하다.

데이터 분석을 통해 일정 수준 이상의 성과를 올리는 기업의 공통 요소
1. 분석 대상의 데이터를 가지고 있으면 분석 기반이 확립되어 있다
2. 데이터 분석의 목적과 현재의 경영 과제를 이해하고 있다
3. 실행팀이 첨단 기술을 획득하려는 의욕으로 가득하며, IT뿐만 아니라 업계 전문 지식이나 적용해야 할 수리 통계, 기계 학습을 올바르게 인식하고 있다
4. 경영층이 리더십을 갖고 프로젝트를 지원하고 있다
5. 시작한 일은 반드시 완수한다는 열정과 기획 발상력이 풍부한 인재를 모아 팀 개인이 아닌 조직의 능력을 최대화시키고 있다

### 미국이 가르쳐 준 데이터 분석

목적에 따라 통계나 분석 수법을 올바르게 실천해 결과를 이끌어내는 것이 중요하며, 이를 위해 분석 담당자는 단순히 분석 자체에 재미를 느끼는 것으로 끝내서는 안된다. (중략) 어디까지나 의사 결정의 최적화를 지원할 뿐이다. 그런데 이 본질을 이해하지 못하는 사람이 많은 것이 현실이다.

구조화 데이터: 회계 데이터나 판매 데이터, 재고 데이터 등 기존형 데이터베이스(관계형 데이터베이스 등)에 형식가 단위를 엄격히 정의해 저장한 데이터.
비구조화 데이터: 텍스트 파일 등의 문서, 영상 등 기존의 데이터베이스에는 담을 수 없는 유형의 데이터.

분석을 무기로 삼는 조직의 정공법
1. 처음부터 전체 최적을 지향한다 -> 모두가 만족하는 시스템을 만드는 것이 바로 프로젝트를 성공시키는 비결
2. 임팩트가 큰 부분부터 착수한다 -> 새로운 프로젝트를 시작할 때 중요한 것은 먼저 일정한 성과를 올리는 일
3. 강력한 리더십의 존재 -> 데이터 분석에서 가장 중요한 것은 복수의 기술 영역을 연결하는 힘
4. 힘의 원천은 팀 편성에서 나온다 -> 최선의 결과를 이끌어내고 싶다면 다양한 인재와 권한 부여형 팀워크가 필요  

---

# [제 2장] 이것만큼은 알아 두자! 기본적인 통계 지식

### '평균', '분산', '표준 편차'를 이해한다.

똑같은 데이터를 봐도 통계에 관한 지식이 있느냐 없느냐에 따라 결론이 크게 달라진다. (중략) 그러므로 통계 데이터를 바탕으로 의사 결정을 할 때는 통계에 관한 최소한의 지식은 반드시 있어야 한다.

도수 분포표: 수집한 미가공 데이터를 일정한 범위에 따라 구분해 집계한 것이다. 데이터를 나눠서 묶은 범위를 '계급'이라고 부르며, 각 계급에 해당하는 수치를 '도수'라고 한다.
히스토그램: 가로축에 계급, 세로축에 도수를 할당해 그래프로 만든 것이 '히스토그램' 혹은 '도수 분포도'다. 도수 분포표를 히스토그램으로 바꾸면 통계 데이터의 전체적인 모습과 분포 정도를 직감적으로 파악하기가 용이하기 때문에 통계 데이터를 시각화하는 수단으로 히스토그램을 자주 이용한다.

정규 분포(가우스 분포): 좌우 대칭의 종 모양이 특징으로, 이것은 꼭대기에 해당하는 '평균값' 부근에 데이터가 많이 분포함을 나타낸다. 그리고 이 꼭대기에서 좌우로 멀어짐에 따라 데이터의 수가 줄어든다. 또한 수집 데이터 중에서 가장 빈도가 높게 나타나는 '최빈값'과 모든 데이터의 정확히 한가운데를 가리키는 '중앙값'이 종의 꼭대기인 '평균값'과 일치한다.
표준 정규 분포: 가로축의 중앙에 위차한 평균값에서 좌우로 '표준 편차' 하나 분량씩 눈금을 넣음으로써 기재된 수치가 어느 정도의 확률로 나타나는지 알 수 있다.

데이터 분석은 우리가 사는 이 세상에서 볼 수 있는 다양한 '사건이나 현상의 들쭉날쭉함을 간파하는 것'이라고 단언할 수 있다.
'평균', '분산', '표준 편차'는 온갖 통계 수법의 기초를 이루는, 사람의 몸으로 치면 골격에 필적할 만큼 중요한 계산 처리 항목이다.

표준 편차: 평균에서의 들쭉날쭉함을 보기 위한 지표 -> 가장 큰 특징은 특수 사례를 추출할 수 있다는 것
변동 계수: 상대적인 들쭉날쭉함을 보기 위한 지표 -> 표준 편차와는 달리 속성이 다를 경우의 비교에 효과적이다. 지역성이나 상권, 입지 같은 조건에 따라 매출 규모가 크게 좌우될 경우, '변동 계수'를 이용하면 조건이 크게 다른 그룹 간의 비교도 할 수 있다. -> 변동 계수(CV)= 표준 편차/평균

### 알아 두면 도움이 되는 분석 수법

요약 통계량: 데이터 분포의 특징을 나타내는 통계량 -> 요약 통계량을 산출하면 데이터의 치우침이나 형상 등을 알 수 있기 때문에 분석의 초기 단계에서 데이터 분포의 특징을 파악하려는 목적으로 자주 산출한다.
* 평균값: 모든 데이터의 합계를 데이터의 개수로 나눈 값
* 표준 편차와 변동 계수: '데이터가 얼마나 들쭉날쭉한가?'를 파악할 수 있다. 수치가 클수록 데이터가 흩어져 있으며, 반대로 작으면 서로 가까이 붙어 있음을 나타낸다.
* 왜도와 첨도: '데이터 분포의 형상'을 파악할 수 있다. 왜도는 분포의 좌우로 치우친 정도를 나타내며, 첨도는 분포가 얼마나 뽀족한가를 나타낸다.
* 중앙값: '정확히 한가운데의 데이터가 어디에 있는가'를 파악할 수 있다. 평균값의 경우, 극단값의 영향을 많이 받기 때문에 실태를 파악하고 싶을 때는 평균값 대신 중앙값을 사용하기도 한다.
* 최빈값: '가장 많은 그룹이 어디에 있는가?'를 파악할수 있다.

빈도주의 통계: 임의의 사건이 발생하는 빈도(빈도 분포)를 확률이라고 정의하는 발상
베이지안 확률: 주어진 정보를 바탕으로 그 사건이 일어날 수 있는 확률의 분포를 갱신해 나간다는 것이다. 또한 분석자가 과거의 경험 등을 바탕으로 사전 확률을 자의적으로 자의적으로 정한다. 이것이 이른바 빈도론의 발상과는 크게 다른 점이다.
* 주된 용도)스팸 메일의 필터링 -> 몇 가지 단에에 대해 '스팸 메일에 들어 있을 확률'과 '정상적인 메일에 들어 있을 확률'을 사전에 산출한 다음 새로운 메일에 들어있는 단어를 조사해 수시로 확률을 갱신한다.

상관분석: 두 데이터이 연계성을 분석하는 수법
* 상관 분석 지표 -> 상관 계수: 범위[-1...+1], +1에 가까울 수록 양의 상관관계, -1에 가까울 수록 음의 상관관계
* 주된 용도) 서비스 해약자의 경향 분석 -> 자사 서비스의 해약 유무에 관한 정보와 고객 속성 정보를 상관 분석해 서비스 해약을 하기 쉬운 사람의 경향을 파악한다.

K-평균법: 클러스터링 기법의 일종으로, 표본을 어떤 기준에 따라 그룹으로 나누고 싶을 때 사용한다. 그룹 분류의 기준은 분석자가 스스로 결정해야 하며, 고객 속성(연령, 가족 수, 소득 등)이나 이용 이력(구매 횟수, 구매 빈도, 구매 상품 수 등)을 조합해 설정한다.
* 주된 용도) 고객의 세분화 -> 고객의 속성 정보나 서비스 이용 이력 등을 기준으로 특성에 입각해 분류할 수 있게 된다. 매출에 공헌하는 우량 고객의 세분화 등에 활용할 수 있다.

협업 필터링: 수많은 사용자의 행동 이력을 바탕으로 다른 사용자의 기호를 예측하는 분석 수법이다.
* 주된 용도) 상품 추천 -> 아마존을 비롯해 많은 인터넷 사이트가 정확도 높은 추천 정보를 이끌어내 판매 촉진에 활용하고 있다.

분산 분석: '집단 내의 들쭉날쭉함'와 '집단 간의 들쭉날쭉함'을 비교한 다음 그 비를 기준으로 요인인 '집단'에 의미가 있는지를 판정한다.
* 주된 용도) 캠페인의 효과 측정 -> 쿠폰 배포나 가격 인하 등 캠페인의 내용이나 실시 유무 등의 조건에 따라 각각의 효과를 측정한다.

다중 회귀 분석: 복수의 데이터를 바탕으로 어떤 하나의 데이터를 예측하는 모델이다. 예측 모델의 대표라고 할 수 있을 만큼 자주 이용된다.
* 모델식: 좌변에는 종속 변수인 '예측 대상'을 우변에는 독립 변수(설명 변수)인 예측에 사용되는 데이터가 들어간다.
* 주된 용도) 판매 예측 -> 과거의 판매 경향이나 소셜 미디어의 반응 수를 바탕으로 판매 수를 예측한다. 그리고 예측 결과를 바탕으로 발주량을 조정해 재고 부족 또는 재고 과다 문제를 개선한다.

로지스틱 회귀 분석: 어떤 사건의 발생 활률을 예측하는 분석 수법이다. 예측을 할 때 다중 회귀 분석과 마찬가지로 복수의 독립 변수를 곱해서 발생 확률을 산출한다. 다만 로지스틱 회기 분석의 식은 다중 회귀 분석과 달리 발생 확률이 0~100퍼센트의 범위에 수렴하도록 만들어졌다.
* 주된 용도) 구매율 예측 -> 상품별로 구매 실적과 고객 속성을 바탕으로 로지스틱 회귀 모델을 만든다. 이를 통해 신규 고객이 구매할 확률이 가장 높은 상품을 제안할 수 있게 된다.

> 데이터 분석은 그 성질상 적지않은 오류를 동반함을 인식하고 오차를 최소한으로 억제하고자 최선을 다하는 분석자야말로 신롸할 수 있는 사람이라고 생각한다.

---

# [제 3장] 데이터 분석을 실천한다

### 데이터 분석의 실천 요건

'실패가 두려워 일단은 작게 시작하는' 것과 '거대한 표적을 정한 다음 우선순위를 정하고 이에 따라 작게 시작하는'것은 비슷해 보이지만 전혀 다르다. 그러므로 회사는 도달하고 싶은 도착점과 그 도착점에 도달하기 위한 목표를 시작 단계에서 명확히 정해야 한다.

로켓 발사에 비유한 데이터 분석 -> 특히 중요한 것은 '발사대'와 '착지점'
* 로켓: 데이터 분석 프로젝트
* 발사대: 데이터 분석을 실시하기 위한 기본적인 요소
* 착지점: 경영 과제의 결정(도착점)
* 발사 책임자: 프로젝트를 승인하고 비호하는 경영 총수
* 선장: 통계의 기초를 이해한 프로젝트 리더
* 승무원: 데이터 분석팀을 구성하는 IT 엔지니어, 해석 담당자, 기획 입안자 등

데이터 분석의 토대(발사대)를 구성하는 6요소
1. 목적(기획 구성)의 입안
2. 데이터 분석을 통해 해결해야 할 과제의 인식과 우선순위 결정
3. 구조화 데이터, 센서 정보나 로그 데이터 등 취득할 IT 기반 -> 가령 병렬 분산 처리라면 하둡, 리얼타임으로 대량 데이터를 처리할 때는 플루언트, 스톰, 누적된 데이터를 바탕으로 규칙의 정확도를 높이는 학습 처리에는 머하웃 등 수많은 소프트웨어가 있음
4. 분석 대상이 되는 데이터
5. 데이터를 분석하기 위한 소프트웨어 및 하드웨어 환경
6. 과제 해결을 향한 리더십과 열정

데이터 분석을 하는 의미는 데이터에서 기업 전체의 성과를 높일 방법(착지점)을 이끌어내 실현하는 데 있다. 설령 프로젝트의 제1단계는 특정 부서나 사업부만의 부분 최적이라도 최종적으로는 기업의 전체 최적을 지향해야 한다.
* 심슨의 역설(Simpson's Paradox'): 모집단의 상관관계와 모집단을 분할한 집단의 상관관계가 크게 다른 경우 발생 -> 이것은 통계에서 부분 최적의 리스크이며, 대국적으로 사물을 측정하는 것이 중요함

사람은 데이터가 올바르다고 해서 반드시 움직이지는 않는다.
사람을 움직이려면 다음 세 가지가 필요하다.
1. 경영층이 현장에 프로젝트의 우선도를 알린다. 그리고 아울러 철저하고 공정한 인사 평가 시스템을 정비한다.
2. 성공했을 때 가장 강렬한 인상을 줄 수 있는 영역부터 착수한다.
3. 데이터가 암시하는 가설을 구체적인 시책에 반영하기 전에 현장 담당자를 만나 이해하고 수긍할 때까지 설명한다.

프로젝트 리더(선장)에게 요구되는 세 가지 능력
1. 프로젝트 전체를 내다보는 구상력
2. 주위의 참여를 이끌어내는 현장력 -> 경영층과 현장의 마음을 파고드는 말로 양자를 연결
3. 통계학과 데이터 분석의 기초적 지식

경영상의 과제 해결을 목적으로 삼는 데이터 과학자의 경우 경영 간부와의 커뮤니케이션은 물론이고 현장과의 커뮤니케이션을 얼마나 확실히 할 수 있느냐에 따라 실행력과 영향도가 크게 달라진다.

데이터 과학자가 하는 일(전체 100%)
1. 분석 목적 정의(14%) -> 분석 목적 정의와 데이터 요건 정의 과정에서는 의견 청취 등의 대인 업무가 중요
2. 데이터 요건 정의(35%)
3. 데이터 가공(24%)
4. 데이터 검색(17%)
5. 분석 모델 구축(10%)

###  데이터 분석을 경영에 활용한다.

데이터 분석을 성공시키기 위한 5단계

### 단계1) 필요한 멤버를 모은다

리더십을 확보해 '개개인'으로서만이 아니라 하나의 팀으로서 유기적으로 기능하는 데이터 분석팀을 조직한다.
* 특출한 능력이 있는 멤버를 고른다. -> 어떤 한 가지 분야에 뛰어난 멤버를 선발
* 데이터 분석은 다양성을 지향한다. -> 한 가지 시점에 집착하지 않고 복수의 관점에서 가설을 구축하고 분석할 수 있다.
* 실적보다 잠재력을 중시한다. -> 설령 직접적인 경험이 없더라도 도전하고자 하는 강한 의욕과 그 의욕을 뒷받침할 능력이 있음을 증명할 수 있는 사람을 선발

### 단계2) 목적을 정한다

기획과 가설 구상력을 갖추고 그 기획의 오더 오브 매그니튜드(영향도)를 살피면서 의사 결정에 필요한 분석을 하기 위한 '출발점'과 '착지점'을 정의한다.
* 도착점과 목표는 비슷하면서도 다르다. -> 예를 들어 '3년 이내에 매출 규모를 두 배, 업계 순위를 3위로 끌어올린다.'는 도착점에 해당한다. 한편 목표는 효율적으로 도착점에 다가가기 위한 수단으로, '히트 상품을 만든다','판매가 부진한 상품의 개선' 등도 포함된다. 그리고 이 이것을 얼마나 달생했는지 측정하는 것이 KPI라고 이해하면 무방하다.
* 데이터 분석의 지향점은 투자를 최대화하기 위한 방책을 높은 정확도로 찾아내는 데 있다. 요컨대 우선순위가 제일 높고 똑같은 금액을 투자했을 때 효과를 내기가 가장 용이한 장소를 노리는 것이다.

### 단계3) 데이터를 처리한다

착지점에 도달하기 위한 데이터 해석을 하는 데 필요한 추출, 변환, 집약 처리 방식을 결정해 실행한다.
* 퀵윈(Quick Win): 데이터가 충분치 않더라도 부분적으로 접근하는 분석 방법 -> 예를 들어 데이터 건수가 행렬 밀도를 지나치게 희석시키지 않을 정도의 데이터 세트 수를 확보하면서 무작위 추출을 감당할 수 있는 표본 수를 얻을 수 있는 수준이라면 지금 있는 데이터로 모델링이 가능
* '시험판' 데이터 분석: 현재의 데이터량과 정확도를 가지고 도착점으로 이어지는 작은 규모의 분석을 실시한 다음 그 효과를 보는 것 -> 유의성을 인정받으면 다음에는 데이터의 양을 늘리고 장기적인 데이터 품질 향상을 위해 정확도를 본질적으로 높이기 위한 데이터를 수집, 축적하는 시스템을 정비해 프로젝트 규모를 키워나갈 수 있음
* 예상 가능한 리스크 검토: 데이터에서 비롯되는 리스크는 데이터 분석 프로젝트의 성패를 크게 좌우한다.

데이터에서 비롯되는 리스크의 점검표
* 분석 목적과 KPI를 고려한 데이터인가?
* 데이터의 추출원이 하나인가? 또는 각 추출원이 나타내는 변수 관계를 신뢰할 수 있는가?
* 분석을 실시하기에 충분한 양의 데이터가 있는가?
* 수집 상황, 조건, 배경 등을 신뢰할 수 있는 질 높은 데이터인가?
* 데이터에 문제가 발생했을 때 대응 가능한 팀 체제인가?
* 개인 정보 보호 등 데이터의 책임 소재를 명확히 하고 있는가?

### 단계4) 모델링을 한다

가설을 바탕으로 검증과 모델을 구축하고 기계 학습의 기초를 형성해 이론과 운용을 연결한다.
* 해결의 실마리는 현장에 있다: 데이터에서 도출된 상관관계와 현장에서 축적된 경험, 지혜를 결합해 더욱 정확도 높은 가설을 구축 -> 분석자는 올바른 분석과 가설입안의 힌트를 얻기 위해 현장과 커뮤니케이션을 하고 탐색적 자료 분석으로 데이터를 다시 한 번 냉정하게 바라봐야 한다.
* 모델링: 사건이나 현상을 수식으로 표현하는 것 -> 아직 일어나지 않은 미지의 사건의 경우 과거의 통계를 바탕으로 확률 개념을 도입한 수리 모델을 설계
* 오차: 예측된 극단값과 실제 값과의 괴리[오차 = 실측값(혹은 관측값) - 극단값] -> 통계 모델링을 할 때 항상 대상과의 괴리(특히 근사화), 즉 오차를 의식하면서 실제 업무 운영이나 의사 결정 프로세스를 설계해야 함

### 단계5) 운용을 최적화한다

실천과 검증을 반복하며 시행착오를 통해 업무의 시점에서 운용을 최적화, 고도화한다.
* 유의성을 시험하기 위해 다시 현장으로: 데이터와 현장의 견식을 바탕으로 구축한 가설을 현장에서 시행하며 가설의 타당성과 방책에 대해 검증을 거듭한다. -> 가설을 운용에 반영할 때 중요한 점은 현장의 경험칙이나 감 같은 '암묵지'를 데이터 분석으로 증명된 '형식지'로 보충한다는 사고방식이다. 우열을 정하는 것이 아니라 쌍방의 혼합이 요구된다. 그리고 이를 위해서는 분석팀과 현장이 하나가 되어 최적화된 새로운 업무 프로세스로 처음에 정했던 도착점에 도달해야 한다.
* 예측 모델이나 업무 프로세스는 그것이 아무리 많은 시간과 노력을 쏟아 부어 만든 것이라 해도 운용하는 과정에서 다시 만들거나 재검토를 해야 한다.

---

# [마무리] 확대되는 데이터 분석의 영역

### 데이터 분석이 안전과 풍요를 가져온다.

논리 수리 모델에서 실측값을 산출할 때는 수식에 오차항을 넣는다. 이 오차가 중요한 의미를 지니는 영역이 있기 때문이다. 공공 정책은 특히 그 개념을 고려해야 하는 영역이다.

사후적 접근: 사건이 발생했을 때, 즉시 상황을 파악하고 정확한 분석을 실시하는 시스템 -> 지금까지는 발생을 예측하는 접근법이 주류였지만, 컴퓨터 성능의 극적인 향상과 센서 데이터의 발전으로 전에는 구축 자체가 불가능했던 즉시 상황 파악 시스템의 구축이 가능해지고 있다.

데이터 분석의 효과는 단순한 비용 절감에 그치지 않는다. 그때 까지 낭비하고 있었던 예산을 좀더 많은 사람이 필요로 하는 사업에 효율적으로 투입하거나 서비스의 질을 전체적으로 향상시킬 수 있게 된다.
